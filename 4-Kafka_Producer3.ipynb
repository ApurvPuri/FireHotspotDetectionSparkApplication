{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Producer 3\n",
    "### Simulating real-time data using Apache Kafka Producer\n",
    "\n",
    "Required library: geohash\n",
    "Installation command: pip3 install geohash\n",
    "\n",
    "\n",
    "First, import all required libraries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, date\n",
    "from pprint import pprint\n",
    "import geohash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then initial the data type for each columns in two input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_attr = ['air_temperature_celcius', 'surface_temperature_celcius', 'confidence', ]\n",
    "float_attr = ['relative_humidity', 'windspeed_knots', 'max_wind_speed', 'latitude', 'longitude']\n",
    "date_arrt = ['date']\n",
    "datetime_attr = ['datetime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will read the CSV data and transform each record into a python dict by using the column name on the first line as a key for the python dictionary. Then all dictionary object of record will be stored in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2dict(file_path, delim):\n",
    "    \n",
    "    header_names = []\n",
    "    is_header_first = True\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for line in open(file_path):\n",
    "        tmp_row = {}\n",
    "        \n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "            \n",
    "        if is_header_first:\n",
    "            header_names = line.split(delim)\n",
    "            is_header_first = False\n",
    "            continue\n",
    "                \n",
    "        tmp_data = line.split(delim)\n",
    "\n",
    "        for i in range(len(header_names)):\n",
    "            #the string data will be re-cating into the right data type using the column name and data type that declared on above  \n",
    "            if header_names[i].strip() in integer_attr:\n",
    "                tmp_row[header_names[i].strip()] = int(tmp_data[i].strip())\n",
    "            elif header_names[i].strip() in float_attr:\n",
    "                tmp_row[header_names[i].strip()] = float(tmp_data[i].strip())\n",
    "            elif header_names[i].strip() in date_arrt:\n",
    "                tmp_row[header_names[i].strip()] = datetime.strptime(tmp_data[i].strip(), '%d/%m/%Y')\n",
    "            elif header_names[i].strip() in datetime_attr:\n",
    "                tmp_row[header_names[i].strip()] = datetime.strptime(tmp_data[i].strip(), '%Y-%m-%dT%H:%M:%S')\n",
    "            else :\n",
    "                tmp_row[header_names[i].strip()] = tmp_data[i].strip()\n",
    "                \n",
    "        result.append(tmp_row)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspot_terra_streaming = \"./hotspot_TERRA_streaming.csv\"\n",
    "hotspot_terra_streaming_dict = csv2dict(hotspot_terra_streaming, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in hotspot_terra_streaming_dict:\n",
    "    item.update({\"sender_id\": \"3\"})\n",
    "    item.update({\"geo_hash\": geohash.encode(item[\"latitude\"], item[\"longitude\"], precision=5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n",
      "Message published successfully. Data: {\"geo_hash\": \"r1w4w\", \"created_time\": \"2019-05-24 15:13:28.114150\", \"sender_id\": \"3\", \"latitude\": -36.1214, \"confidence\": 59, \"surface_temperature_celcius\": 40, \"longitude\": 143.7078}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r38c0\", \"created_time\": \"2019-05-24 15:13:45.150379\", \"sender_id\": \"3\", \"latitude\": -36.3631, \"confidence\": 73, \"surface_temperature_celcius\": 53, \"longitude\": 147.3135}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r1kq0\", \"created_time\": \"2019-05-24 15:13:57.175084\", \"sender_id\": \"3\", \"latitude\": -36.9034, \"confidence\": 100, \"surface_temperature_celcius\": 95, \"longitude\": 141.0013}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r1x5t\", \"created_time\": \"2019-05-24 15:14:20.187367\", \"sender_id\": \"3\", \"latitude\": -35.9423, \"confidence\": 79, \"surface_temperature_celcius\": 52, \"longitude\": 145.0943}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r328v\", \"created_time\": \"2019-05-24 15:14:36.226460\", \"sender_id\": \"3\", \"latitude\": -37.8265, \"confidence\": 76, \"surface_temperature_celcius\": 49, \"longitude\": 147.1984}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r1nqe\", \"created_time\": \"2019-05-24 15:14:56.259041\", \"sender_id\": \"3\", \"latitude\": -38.2093, \"confidence\": 57, \"surface_temperature_celcius\": 40, \"longitude\": 143.9237}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r1rer\", \"created_time\": \"2019-05-24 15:15:09.280045\", \"sender_id\": \"3\", \"latitude\": -37.3851, \"confidence\": 83, \"surface_temperature_celcius\": 63, \"longitude\": 145.8878}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r1wh2\", \"created_time\": \"2019-05-24 15:15:22.304982\", \"sender_id\": \"3\", \"latitude\": -35.7786, \"confidence\": 84, \"surface_temperature_celcius\": 58, \"longitude\": 143.469}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r1nx3\", \"created_time\": \"2019-05-24 15:15:52.323334\", \"sender_id\": \"3\", \"latitude\": -38.0771, \"confidence\": 52, \"surface_temperature_celcius\": 38, \"longitude\": 144.2044}\n",
      "Message published successfully. Data: {\"geo_hash\": \"r1tdx\", \"created_time\": \"2019-05-24 15:16:04.349097\", \"sender_id\": \"3\", \"latitude\": -36.1031, \"confidence\": 63, \"surface_temperature_celcius\": 45, \"longitude\": 143.0782}\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "from time import sleep\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import socket\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "lines = hotspot_terra_streaming_dict\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully. Data: ' + json.dumps(line))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'],\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    topic = 'fire'\n",
    "    \n",
    "    print('Publishing records..')\n",
    "    producer = connect_kafka_producer()\n",
    "    while(True):\n",
    "        line = lines[random.randrange(len(lines))]\n",
    "        line.update({\"created_time\":str(datetime.datetime.now())})  \n",
    "#     print(line)\n",
    "        publish_message(producer, topic, 'parsed', json.dumps(line))\n",
    "        sleep(random.randrange(10,31,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
