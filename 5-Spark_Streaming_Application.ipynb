{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pprint import pprint\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-define function for writing the data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sendDataToDB(raw_record):\n",
    "    client = MongoClient()\n",
    "    db = client.streaming_db\n",
    "    climate_collection = db.climate\n",
    "    \n",
    "    for tmp_tuple in raw_record:\n",
    "        data = {\"climate\":[],\n",
    "                \"hotspot\":[],\n",
    "               }\n",
    "\n",
    "        hash_key = tmp_tuple[0]\n",
    "        print(\"hash key\", hash_key)\n",
    "        for record in tmp_tuple[1]:\n",
    "            print(\"record\", record)\n",
    "            if record[\"sender_id\"] == '1':\n",
    "                data[\"climate\"].append(record)\n",
    "            else:\n",
    "                data[\"hotspot\"].append(record)\n",
    "                \n",
    "        if len(data[\"climate\"]) > 0:\n",
    "            #pprint(data)\n",
    "            \n",
    "            #DATA MODEL FOR HOTSPOT\n",
    "            hotspot_model = {'confidence': 0.0,\n",
    "              'created_time': None,\n",
    "              'latitude': 0.0,\n",
    "              'longitude': 0.0,\n",
    "              'surface_temperature_celcius': 0.0}\n",
    "            \n",
    "            hotspot_id = None\n",
    "            #create hotspot data model\n",
    "            for tmp_hotspot in data[\"hotspot\"]:\n",
    "                hotspot_model['confidence'] += tmp_hotspot['confidence']\n",
    "                hotspot_model['surface_temperature_celcius'] += tmp_hotspot['surface_temperature_celcius']\n",
    "                #'2019-05-20 18:42:57.622646'\n",
    "                hotspot_model['created_time'] = datetime.strptime(tmp_hotspot['created_time'], '%Y-%m-%d %H:%M:%S.%f')\n",
    "                hotspot_model['latitude'] = tmp_hotspot['latitude']\n",
    "                hotspot_model['longitude'] = tmp_hotspot['longitude']\n",
    "                hotspot_model['_id'] = tmp_hotspot['created_time']\n",
    "                hotspot_id = tmp_hotspot['created_time']\n",
    "            \n",
    "            \n",
    "            #average confidence and surface_temperature_celcius\n",
    "            if hotspot_id is not None:\n",
    "                hotspot_model['confidence'] = hotspot_model['confidence']/float(len(data[\"hotspot\"]))\n",
    "                hotspot_model['surface_temperature_celcius'] = hotspot_model['surface_temperature_celcius']/float(len(data[\"hotspot\"]))\n",
    "\n",
    "                \n",
    "                \n",
    "            #create climate data model\n",
    "            for tmp_climate in data[\"climate\"]:\n",
    "                #DATA MODEL FOR CLIMATE\n",
    "                climate_model = {'air_temperature_celcius': tmp_climate['air_temperature_celcius'],\n",
    "                                 'created_time': datetime.strptime(tmp_climate['created_time'], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "                                 'latitude': tmp_climate['latitude'],\n",
    "                                 'longitude': tmp_climate['longitude'],\n",
    "                                 'max_wind_speed': tmp_climate['max_wind_speed'],\n",
    "                                 'precipitation': tmp_climate['precipitation'],\n",
    "                                 'relative_humidity': tmp_climate['relative_humidity'],\n",
    "                                 'hotspots': [hotspot_model] if hotspot_model['created_time'] is not None else [],    #[hotspot_id] if hotspot_id is not None else [],\n",
    "                                 'windspeed_knots': tmp_climate['windspeed_knots'],\n",
    "                                 '_id': tmp_climate['created_time']\n",
    "                                }\n",
    "                \n",
    "                print(\"Saving climate to mongo\", climate_model)                \n",
    "                \n",
    "                try:\n",
    "                    climate_collection.insert(climate_model)\n",
    "                except Exception as ex:\n",
    "                    print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "                \n",
    "                \n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Function to get stream from Kafka Producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting....\n",
      "Creating streaming connections\n",
      "fetching.....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting....\")\n",
    "\n",
    "n_secs = 10\n",
    "topic = \"fire\"\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "\n",
    "print(\"Creating streaming connections\")\n",
    "\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'climate', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "\n",
    "print(\"fetching.....\")\n",
    "\n",
    "#mapping geohash with the records\n",
    "parsed = kafkaStream.map(lambda v: (json.loads(v[1])[\"geo_hash\"], json.loads(v[1])))\n",
    "#parsed.pprint()\n",
    "\n",
    "#groupping the record based on the geohash\n",
    "group = parsed.groupByKey()\n",
    "#group.pprint()\n",
    "\n",
    "#appling the function to each partition of each RDD in the stream\n",
    "group.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
