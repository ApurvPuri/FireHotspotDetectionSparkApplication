{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Producer 2 \n",
    "### Simulating real-time data using Apache Kafka Producer\n",
    "\n",
    "Required library: geohash\n",
    "Installation command: pip3 install geohash\n",
    "\n",
    "\n",
    "First, import all required libraries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, date\n",
    "from pprint import pprint\n",
    "import geohash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then initial the data type for each columns in input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_attr = ['air_temperature_celcius', 'surface_temperature_celcius', 'confidence', ]\n",
    "float_attr = ['relative_humidity', 'windspeed_knots', 'max_wind_speed', 'latitude', 'longitude']\n",
    "date_arrt = ['date']\n",
    "datetime_attr = ['datetime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will read the CSV data and transform each record into a python dict by using the column name on the first line as a key for the python dictionary. Then all dictionary object of record will be stored in a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2dict(file_path, delim):\n",
    "    \n",
    "    header_names = []\n",
    "    is_header_first = True\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for line in open(file_path):\n",
    "        tmp_row = {}\n",
    "        \n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "            \n",
    "        if is_header_first:\n",
    "            header_names = line.split(delim)\n",
    "            is_header_first = False\n",
    "            continue\n",
    "                \n",
    "        tmp_data = line.split(delim)\n",
    "\n",
    "        for i in range(len(header_names)):\n",
    "            #the string data will be re-cating into the right data type using the column name and data type that declared on above  \n",
    "            if header_names[i].strip() in integer_attr:\n",
    "                tmp_row[header_names[i].strip()] = int(tmp_data[i].strip())\n",
    "            elif header_names[i].strip() in float_attr:\n",
    "                tmp_row[header_names[i].strip()] = float(tmp_data[i].strip())\n",
    "            elif header_names[i].strip() in date_arrt:\n",
    "                tmp_row[header_names[i].strip()] = datetime.strptime(tmp_data[i].strip(), '%d/%m/%Y')\n",
    "            elif header_names[i].strip() in datetime_attr:\n",
    "                tmp_row[header_names[i].strip()] = datetime.strptime(tmp_data[i].strip(), '%Y-%m-%dT%H:%M:%S')\n",
    "            else :\n",
    "                tmp_row[header_names[i].strip()] = tmp_data[i].strip()\n",
    "                \n",
    "        result.append(tmp_row)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspot_aqua_streaming = \"./hotspot_AQUA_streaming.csv\"\n",
    "hotspot_aqua_streaming_dict = csv2dict(hotspot_aqua_streaming, ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in hotspot_aqua_streaming_dict:\n",
    "    item.update({\"sender_id\": \"2\"})\n",
    "    item.update({\"geo_hash\": geohash.encode(item[\"latitude\"], item[\"longitude\"], precision=5)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n",
      "Message published successfully. Data: {\"confidence\": 50, \"latitude\": -37.3556, \"created_time\": \"2019-05-24 15:17:48.746432\", \"geo_hash\": \"r1rgk\", \"sender_id\": \"2\", \"surface_temperature_celcius\": 47, \"longitude\": 146.1085}\n",
      "Message published successfully. Data: {\"confidence\": 79, \"latitude\": -36.2987, \"created_time\": \"2019-05-24 15:18:16.764223\", \"geo_hash\": \"r1s3e\", \"sender_id\": \"2\", \"surface_temperature_celcius\": 52, \"longitude\": 141.1294}\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "from time import sleep\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "lines = hotspot_aqua_streaming_dict\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully. Data: ' + json.dumps(line))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['127.0.0.1:9092'],\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "    topic = 'fire'\n",
    "    \n",
    "    print('Publishing records..')\n",
    "    producer = connect_kafka_producer()\n",
    "    while(True): \n",
    "        line = lines[random.randrange(len(lines))]\n",
    "        line.update({\"created_time\":str(datetime.datetime.now())})  \n",
    "#        print(line)\n",
    "        publish_message(producer, topic, 'parsed', json.dumps(line))\n",
    "        sec = random.randrange(10,31,1)\n",
    "        sleep(int(sec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
