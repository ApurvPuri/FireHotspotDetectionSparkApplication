In this task,multiple Apache Kafka producers will be implemented to simulate the real-time streaming of the data which will be processed by Apache Spark Streaming client and then inserted into MongoDB. Simulating real-time data using Apache Kafka Producers. a. Event Producer 1: Write a python program that loads all the data from climate_streaming.csv and randomly feed the data to the stream every 5 seconds. You will need to append additional information such as sender_id and created_time. Save the file as Assignment_TaskC_Producer1.ipynb. b. Event Producer 2: Write a python program that loads all the data from hotspot_AQUA_streaming.csv and randomly feed the data to the stream every 10 - 30 seconds. AQUA is the satellite from NASA that reports latitude, longitude, confidence and surface temperature of a location. You will need to append additional information such as sender_id and created_time. Save the file as Assignment_TaskC_Producer2.ipynb. c. Event Producer 3: Write a python program that loads all the data from hotspot_TERRA_streaming.csv and randomly feed the data to the stream every 10 - 30 seconds. TERRA is another satellite from NASA that reports latitude, longitude, confidence and surface temperature of a location. You will need to append additional information such as sender_id and created_time. Save the file as Assignment_TaskC_Producer3.ipynb. 2. Stream Processing using Apache Spark Streaming. a. Streaming Application: Write a streaming application in Apache Spark Streaming which has a local streaming context with two execution threads and a batch interval of 10 seconds. The streaming application will receive streaming data from all three producers. If the streaming application has data from all or at least two of the producers, do the processing as follows: - Join the streams based on the location (i,e, latitude and longitude) and create the data model developed in Task A. - Find if two locations are close to each other or not. You can do this by implementing the geo-hashing algorithm or find the library that does the job for you. Use precision 5. The precision determines the number of characters in the Geohash. - If we receive the data from two different satellites AQUA and TERRA for the same location, then average the ‘surface temperature’ and ‘confidence’. - If the streaming application has the data from only one producer (Producer 1), it implies that there was no fire at that time and we can store the climate data into MongoDB straight away. Save the file as Assignment_TaskC_Streaming_Application.ipynb. 3. Data Visualisation using MatPlotLib a. Streaming data visualization: i. For the incoming climate data plot the line graph of air temperature against arrival time. You need to label some interesting points such as maximum and minimum values. b. Static data visualization: Write python programs using pymongo to get the data from the MongoDB collection(s) created in Task C.2 and perform the following visualizations. i. Records with the top 10 number of fires. Plot a bar chart with time as the x-axis and number of fires as the y-axis. ii. Plot fire locations in the map with air temperature, surface temperature, relative humidity and confidence